# Assessment Best Practices for Online Asynchronous Graduate Programs (2025-2026)

## Research Summary for MSBAi Program Design

*Compiled: January 12, 2026*

This document synthesizes current research and best practices for assessment in online asynchronous graduate programs, with specific attention to the challenges posed by AI tool availability and recommendations for analytics/data science/business programs.

---

## 1. Why Traditional Exams Are at Risk

### The AI Cheating Challenge

Traditional written exams and take-home assignments face unprecedented challenges in online environments:

**Detection Impossibility**
- Experts agree it is impossible to be completely certain whether a student used AI - the technology is too sophisticated ([Inside Higher Ed](https://www.insidehighered.com/news/faculty-issues/learning-assessment/2025/12/16/you-cant-ai-proof-classroom-experts-say-get))
- Research shows markers cannot distinguish AI-assisted assessments from unassisted ones, regardless of assessment authenticity level ([British Journal of Educational Technology](https://bera-journals.onlinelibrary.wiley.com/doi/full/10.1111/bjet.13585))
- AI detection tools produce high false positive/negative rates and disproportionately harm non-native English speakers

**Emerging Cheating Technologies**
- Tools like Cluely and Parakeet AI provide real-time AI assistance invisible to proctoring
- Meta Ray-Ban glasses with built-in AI can communicate silently via in-lens display ([Washington Post](https://www.washingtonpost.com/education/2025/12/12/ai-artificial-intelligence-college-oral-exam/))
- Screen-sharing workarounds defeat lockdown browsers

**Surveillance Limitations**
- Enforcement tools prioritize surveillance over trust and reward compliance over learning
- Create inequitable outcomes for diverse student populations
- Damage instructor-student relationships essential for online learning success

### Key Insight for MSBAi
> "The seismic shift brought about by generative AI is challenging the fundamental pillars of written coursework assessment in higher education." ([Scientect](https://scientect.org/2025/10/27/the-oral-defence-higher-educations-shield-in-the-generative-ai-world/))

---

## 2. Project-Based Assessment Best Practices

### Leading Program Examples

**University of Chicago - MS Applied Data Science**
- Teams of 4 students work with real business sponsors
- Guided by instructor and subject matter expert
- Clear expectations from sponsor + learning objectives from instructors
- ([UChicago DSI](https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/capstone-projects/))

**Cornell Johnson MSBA**
- Coursework shaped around real datasets, case simulations, and team-based projects
- Designed for immediate workplace application
- ([Cornell Johnson](https://www.johnson.cornell.edu/programs/specialized-masters/ms-in-business-analytics/))

**USC Marshall MSBA**
- Case competitions and cross-functional teams
- Projects with industry professional mentorship
- Portfolio building from day one
- ([USC Marshall](https://www.marshall.usc.edu/programs/graduate-programs/specialized-masters/ms-business-analytics))

**Columbia MSBA**
- Capstone provides intense consulting engagement with clients
- Real-world business problems using real data sets
- ([Columbia DSI](https://datascience.columbia.edu/education/programs/m-s-in-data-science/capstone/))

### Design Principles for Project-Based Assessment

1. **Industry Partnership**: Connect students with real organizations and real problems
2. **Team Composition**: Groups of 3-4 students with faculty mentor supervision
3. **Dual Accountability**: Sponsor expectations + academic learning objectives
4. **Portfolio Integration**: Projects become career assets, not just grades
5. **Presentation Component**: Final delivery includes live presentation/defense

### Recommended Project Assessment Components

| Component | Weight | Purpose |
|-----------|--------|---------|
| Process Documentation | 20% | Evidence of journey, not just outcome |
| Technical Deliverables | 30% | Code, analysis, models |
| Written Report | 20% | Communication and synthesis |
| Oral Presentation/Defense | 30% | Authenticity verification |

---

## 3. Portfolio-Based Assessment Models

### Framework: Competency-Based Portfolios

Portfolio assessment works well in graduate programs because students can demonstrate mastery through accumulated evidence rather than single-point assessments ([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC7283408/)).

**Key Components:**

1. **Artifact Collection**: Students curate work samples demonstrating each competency
2. **Reflection**: Written analysis connecting artifacts to learning outcomes
3. **Progression Evidence**: Show growth over time, not just final state
4. **External Validation**: Industry feedback on portfolio quality

### Cleveland Clinic Medical School Model
- No letter grades or class ranks
- Students receive qualitative assessments
- Nine broad-based competencies tracked through portfolio
- Feedback from faculty and peers used as evidence
- ([PubMed](https://pubmed.ncbi.nlm.nih.gov/17457074/))

### MPA Program Model (NASPAA)
- Competency-based portfolios serve both pedagogical and assessment functions
- Students demonstrate core public administration competencies
- Artifacts aligned to program learning outcomes
- ([NASPAA](https://www.naspaa.org/using-competency-based-portfolios-pedagogical-tool-and-assessment-strategy-mpa-programs))

### Portfolio Structure for MSBAi

| Category | Artifacts | Assessment Focus |
|----------|-----------|------------------|
| Technical Skills | Code repos, analysis notebooks, dashboards | Competency demonstration |
| Communication | Reports, presentations, visualizations | Professional communication |
| Problem-Solving | Case analyses, project decisions | Critical thinking process |
| Collaboration | Team reflections, peer feedback | Professional behavior |
| Growth | Version comparisons, revision history | Learning journey |

---

## 4. Competency-Based and Mastery-Based Assessment

### Core Principles

Competency-based education (CBE) focuses on demonstrating skill mastery rather than seat time ([VerifyEd](https://www.verifyed.io/blog/competency-learning-assessment-guide)).

**Key Features:**
- Students progress when they demonstrate mastery
- Assessment tied to specific, observable competencies
- Multiple opportunities to demonstrate mastery
- Transparent criteria and rubrics

### Programs Leading in CBE

**Capella University FlexPath**
- Self-paced progression through bachelor's, master's, doctoral programs
- Business, healthcare, IT, psychology programs
- Mastery of specific skills and knowledge
- ([MyDegreeGuide](https://www.mydegreeguide.com/competency-based-degrees/))

**South College CBE**
- Competency units earned through exams, portfolios, or capstone projects
- Self-starters and working professionals
- Evidence mastery of required content at own pace
- ([South College](https://www.south.edu/location/cbe-competency-based-education/))

### Implementation for MSBAi

**Competency Categories:**

1. **Data Management** (Database, ETL, Data Quality)
2. **Statistical Analysis** (Inference, Regression, Time Series)
3. **Machine Learning** (Classification, Clustering, Prediction)
4. **Visualization & Communication** (Dashboards, Reports, Presentations)
5. **Business Application** (Problem Framing, ROI Analysis, Recommendations)
6. **Professional Practice** (Ethics, Collaboration, Project Management)

**Mastery Levels:**
- Level 1: Foundational (can execute with guidance)
- Level 2: Proficient (can execute independently)
- Level 3: Advanced (can teach and innovate)

---

## 5. Assessing Learning When Students Can Use AI Tools

### The FACT Assessment Framework

From Frontiers in Education research on environmental data science education ([Frontiers](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2025.1596462/full)):

| Component | AI Usage | Purpose |
|-----------|----------|---------|
| **F**undamental Skills | No AI | Build foundation before advanced concepts |
| **A**pplied Projects | AI-assisted | Real-world problem-solving with AI tools |
| **C**onceptual Understanding | No AI | Paper-based exam for independent comprehension |
| **T**hinking (Critical) | AI + Human | Assess and integrate AI outputs |

**Pedagogical Rationale:**
- Addresses "cognitive paradox of AI in education"
- Sequences instruction from foundational (no AI) to AI-assisted application
- Returns to independent validation before completion
- Aligns with cognitive load theory

### AI Assessment Scale (AIAS)

Framework for defining permitted AI integration levels in each assignment:

| Level | AI Usage | Example |
|-------|----------|---------|
| Level 0 | No AI permitted | In-class exams, oral defenses |
| Level 1 | AI for brainstorming only | Idea generation, not content |
| Level 2 | AI for drafting with human revision | First draft assistance |
| Level 3 | AI as collaborative tool | Full integration with disclosure |
| Level 4 | AI as subject of analysis | Critique and compare AI outputs |

### Process-Product Assessment Model

From faculty training workshops ([Frontiers](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1499495/full)):

**Evaluate both:**
1. **Final Product** - Traditional deliverable quality
2. **Process Documentation**:
   - Prompt development and refinement
   - Human-AI interaction quality
   - Critical evaluation of AI outputs
   - Revision decisions and rationale

### AI Declaration Requirements

Require students to document:
- Which AI tools were used
- What prompts were employed
- What limitations were encountered
- How human judgment modified AI outputs

---

## 6. Peer Review and Collaborative Assessment

### Research-Backed Benefits

MIT research shows learners who provided more peer feedback received better grades ([MIT Open Learning](https://openlearning.mit.edu/news/how-peer-review-enables-better-learning-online-courses)).

Students with high engagement in peer assessment showed:
- Superior instructional design abilities
- More sophisticated cognitive structures
- More positive emotional/behavioral engagement
- Higher-quality cognitive engagement

### Four Pillars Framework

From higher education research ([Springer](https://link.springer.com/chapter/10.1007/978-3-031-29411-2_1)):

| Pillar | Focus |
|--------|-------|
| **Veracity** | Assessment design integrity |
| **Validity** | Implementation accuracy |
| **Volume** | Sufficient feedback quantity |
| **Literacy** | Student skill in giving/receiving feedback |

### Implementation Best Practices

**Transparent Rubrics**
- Provide detailed criteria for peer evaluation
- Train students to apply rubric consistently
- Include workshops or interactive activities

**Technology Platforms**
- Canvas Workshop activity for automated distribution
- Anonymous evaluations to reduce bias
- Moodle, Google Drive, Microsoft Teams integration

**Structured Reflection**
- Post-feedback discussion boards
- Virtual reflection sessions
- Clarifying questions and insight sharing

**Accessibility Considerations**
- Flexible alternatives for students with limited technology
- Technical support sessions
- Collaboration with IT for device lending

### Peer Assessment Types for MSBAi

| Type | Description | When to Use |
|------|-------------|-------------|
| Code Review | Evaluate peer code quality and documentation | Technical courses |
| Analysis Critique | Assess methodology and conclusions | Statistics/ML courses |
| Presentation Feedback | Evaluate communication effectiveness | Capstone, storytelling |
| Team Contribution | Rate collaboration and reliability | Group projects |

---

## 7. Authentic Assessment Mirroring Real Workplace Tasks

### Duke CTL Framework

Six concrete approaches from Duke's Center for Teaching and Learning ([Duke CTL](https://ctl.duke.edu/blog/2025/10/authentic-assessment-over-surveillance/)):

1. **Performance Tasks and Projects**
   - Build prototypes, policy memos, public-facing deliverables
   - Real stakeholder audiences when possible

2. **Case Studies and Simulations**
   - Context-rich problems with incomplete information
   - Require justification of decisions and trade-offs

3. **Guided Investigations**
   - Deep exploration with presentations or extended writing
   - Student-directed inquiry within framework

4. **Oral Defenses**
   - Defend choices, trade-offs, and revisions live
   - Cannot be AI-generated in real-time

5. **Process-Centered Work**
   - Value drafts, logs, notebooks alongside final products
   - Document decision-making journey

6. **Digital Portfolios**
   - Cumulative evidence of growth
   - Annotated with standards-aligned rubrics

### AI-Resistant Authentic Assessment Strategies

**Anchor in Local Context:**
- Local data that AI cannot fabricate
- Lived experiences unique to student
- Recent class discussions and debates
- Current events and organization-specific problems

**Emphasize Process Over Product:**
- Weight iterative steps: drafting, feedback, revision
- Meaningful credit for process documentation
- Reflection on learning journey

**Social-Experiential Learning:**
- Performative rather than output-based
- Synchronous interaction components
- Collaborative work that requires real-time coordination

### Interview/Oral Exam Implementation

From University of Dayton research ([U Dayton](https://udayton.edu/blogs/onlinelearning/2025/25-01-06-the-cheat-proof-assessment.php)):

**Time Efficiency:**
- 30-student class: ~300 minutes total (vs. 450 for grading papers)
- Spread across multiple days using scheduled office hours

**Best Practices:**
1. Clear communication of expectations
2. Example videos showing what to expect
3. Office hours practice sessions
4. Challenging but fair questions aligned to learning objectives
5. Question variation across students
6. Standardized scoring rubric

**Proposed Assessment Weighting Model:**
- Written component: 30% (research, structure, communication)
- Oral defense: 70% (process ownership, authentic understanding)

---

## 8. Specific Recommendations for MSBAi Online Program

### Assessment Philosophy

**Principle 1: Transparency Over Surveillance**
- Clear AI usage policies for each assignment
- Trust-based approach with accountability mechanisms
- Focus on learning outcomes, not compliance

**Principle 2: Process Over Product**
- Document learning journey, not just final deliverables
- Weight revision and iteration
- Reflection as assessment component

**Principle 3: Authentic Application**
- Real data, real problems, real stakeholders
- Industry partnerships for capstone projects
- Portfolio building throughout program

**Principle 4: Multiple Assessment Modes**
- Combine written, oral, practical, collaborative
- No single high-stakes assessment determines outcome
- Competency demonstrated through varied evidence

### Course-Level Assessment Design

**For Technical Courses (Database, BI, ML):**

| Component | Weight | AI Policy |
|-----------|--------|-----------|
| Labs/Exercises | 20% | No AI (foundation building) |
| Projects | 40% | AI-assisted with disclosure |
| Technical Quiz (oral or proctored) | 20% | No AI |
| Peer Code Review | 10% | N/A |
| Portfolio Documentation | 10% | AI for editing only |

**For Communication Courses (Data Storytelling):**

| Component | Weight | AI Policy |
|-----------|--------|-----------|
| Presentations (recorded + live) | 30% | AI for preparation, not delivery |
| Written Analysis | 25% | AI-assisted with disclosure |
| Peer Feedback | 15% | N/A |
| Revision Portfolio | 20% | Show before/after with reflection |
| Final Oral Defense | 10% | No AI |

**For Capstone:**

| Component | Weight | AI Policy |
|-----------|--------|-----------|
| Sponsor Deliverables | 30% | Industry-standard (AI permitted) |
| Process Documentation | 20% | AI for editing only |
| Final Presentation | 25% | No AI during delivery |
| Oral Defense/Q&A | 15% | No AI |
| Peer Evaluation | 10% | N/A |

### Program-Level Portfolio Structure

**Required Artifacts by Semester:**

| Semester | Artifacts | Competencies Demonstrated |
|----------|-----------|--------------------------|
| 1 | SQL projects, data visualization, reflection | Database, visualization basics |
| 2 | ML models, business case analyses | Predictive modeling, business application |
| 3 | Team project deliverables, peer reviews | Collaboration, communication |
| 4 | Capstone + comprehensive reflection | Integration, professional readiness |

### Synchronous Assessment Components

Even in async-first programs, include synchronous touchpoints:

1. **Monthly Webinar Discussions** (participation tracked)
2. **Mid-term Check-ins** (15-min instructor conversation)
3. **Project Presentations** (live via Zoom, recorded backup)
4. **Capstone Defense** (mandatory synchronous, panel format)

### Technology Stack for Assessment

| Tool | Purpose |
|------|---------|
| Canvas | Assignment submission, rubrics, peer review |
| GitHub | Code portfolios, version history |
| Zoom | Oral defenses, presentations |
| Gradescope | Code autograding with manual review |
| Peergrade | Structured peer assessment |
| Portfolio Platform (custom or Portfolium) | Cumulative evidence |

---

## 9. Implementation Checklist

### Before Launch
- [ ] Define AI usage policy for each course and assignment type
- [ ] Create AI Assessment Scale levels for program
- [ ] Develop standardized rubrics including process components
- [ ] Train faculty on oral defense/interview assessment
- [ ] Set up portfolio platform infrastructure
- [ ] Establish industry partnerships for authentic projects

### Per Course
- [ ] Map assessments to competencies
- [ ] Balance AI-permitted and AI-free components
- [ ] Include at least one synchronous assessment
- [ ] Integrate peer review/feedback mechanisms
- [ ] Create clear process documentation requirements
- [ ] Design reflection prompts for portfolio

### Quality Assurance
- [ ] Regular calibration sessions for oral assessment scoring
- [ ] Inter-rater reliability checks on portfolio evaluation
- [ ] Student feedback on assessment fairness and clarity
- [ ] Industry partner feedback on graduate preparedness
- [ ] Annual assessment framework review

---

## 10. Sources and Further Reading

### Academic Research
- [Frontiers: AI-resistant assessments from faculty training workshops](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1499495/full)
- [Frontiers: FACT Assessment Framework](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2025.1596462/full)
- [British Journal of Educational Technology: GenAI impact on authentic assessment](https://bera-journals.onlinelibrary.wiley.com/doi/full/10.1111/bjet.13585)
- [PMC: Student perspectives on competency-based portfolios](https://pmc.ncbi.nlm.nih.gov/articles/PMC7283408/)
- [Springer: Four Pillars of Peer Assessment](https://link.springer.com/chapter/10.1007/978-3-031-29411-2_1)

### University Best Practices
- [Duke CTL: Authentic Assessment Over Surveillance](https://ctl.duke.edu/blog/2025/10/authentic-assessment-over-surveillance/)
- [University of Dayton: Cheat-Proof Assessment](https://udayton.edu/blogs/onlinelearning/2025/25-01-06-the-cheat-proof-assessment.php)
- [University of Saskatchewan: AI-Resistant Oral Assessment](https://teaching.usask.ca/articles/2025-02-28-ai-resistant-oral-assessment.php)
- [MIT Open Learning: Peer Review in Online Courses](https://openlearning.mit.edu/news/how-peer-review-enables-better-learning-online-courses)
- [Columbia CTL: Peer Review Design](https://ctl.columbia.edu/resources-and-technology/resources/peer-review/)

### Program Examples
- [UChicago Data Science Capstone](https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/capstone-projects/)
- [Cornell Johnson MSBA](https://www.johnson.cornell.edu/programs/specialized-masters/ms-in-business-analytics/)
- [USC Marshall MSBA](https://www.marshall.usc.edu/programs/graduate-programs/specialized-masters/ms-business-analytics)
- [Columbia MSBA Capstone](https://datascience.columbia.edu/education/programs/m-s-in-data-science/capstone/)
- [Virginia Data Science Capstone](https://datascience.virginia.edu/degrees/info/capstone-projects)

### Industry and Policy
- [Inside Higher Ed: AI-Proofing the Classroom](https://www.insidehighered.com/news/faculty-issues/learning-assessment/2025/12/16/you-cant-ai-proof-classroom-experts-say-get)
- [Washington Post: Oral Exams to Combat AI](https://www.washingtonpost.com/education/2025/12/12/ai-artificial-intelligence-college-oral-exam/)
- [Times Higher Education: Peer Review in Online Courses](https://www.timeshighereducation.com/campus/how-encourage-peer-review-online-courses)
- [VerifyEd: Competency-Based Learning Guide 2025](https://www.verifyed.io/blog/competency-learning-assessment-guide)
- [Thesify: Student AI Survey 2025](https://www.thesify.ai/blog/2025-student-ai-survey-insights-ai-tools-for-students-in-higher-education)

---

*Document created for MSBAi Program Development - Gies College of Business*
